{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjJBYTUg0vWl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s02yrSoUcfxj"
      },
      "source": [
        "## Vocabulary building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- download data from [here](https://drive.google.com/file/d/1oD6R-JW4muQ38VG3HNBDW4Z31Bx5f4B3/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eOR3oatZ1AsU",
        "outputId": "90fac793-69fd-429c-a058-a2dd2cb210c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total articles: 13379\n",
            "Minimum document count: 133 (appears in 1.0% of articles)\n",
            "Final vocabulary size: 8603\n",
            "Vocabularies created and saved successfully!\n"
          ]
        }
      ],
      "source": [
        "SPECIAL_TOKENS = {\n",
        "    \"<pad>\": 0,  # Padding\n",
        "    \"<sos>\": 1,  # Start of sequence\n",
        "    \"<eos>\": 2,  # End of sequence\n",
        "    \"<unk>\": 3   # Unknown word\n",
        "}\n",
        "\n",
        "\n",
        "def build_vocab(data, min_freq_ratio=0):\n",
        "    # Count how many articles each token appears in\n",
        "    token_document_count = Counter()\n",
        "    total_articles = len(data)\n",
        "\n",
        "    print(f\"Total articles: {total_articles}\")\n",
        "\n",
        "    for article in tqdm(data, desc='processing articles...'):\n",
        "        # Get unique tokens from this article\n",
        "        # Using set() ensures each token is counted only once per article\n",
        "        # even if it appears multiple times in the text\n",
        "        text_tokens = set(article['text'].split())\n",
        "        title_tokens = set(article['title'].split())\n",
        "        unique_tokens = text_tokens | title_tokens\n",
        "\n",
        "        # Increment the counter for each unique token in this article\n",
        "        token_document_count.update(unique_tokens)\n",
        "\n",
        "    # Calculate minimum document frequency threshold based on percentage\n",
        "    min_document_count = max(1, int(min_freq_ratio * total_articles))\n",
        "    print(f\"Minimum document count: {min_document_count} (appears in {min_freq_ratio*100:.1f}% of articles)\")\n",
        "\n",
        "    # Create vocabulary with tokens that appear in at least min_document_count articles\n",
        "    # This ensures we only keep tokens that appear in the specified percentage of articles\n",
        "    vocab = {\n",
        "        word: i + len(SPECIAL_TOKENS)\n",
        "        for i, (word, count) in tqdm(\n",
        "            enumerate(token_document_count.items()),\n",
        "            desc='creating vocabulary'\n",
        "        )\n",
        "        if count >= min_document_count\n",
        "    }\n",
        "\n",
        "    vocab = {**SPECIAL_TOKENS, **vocab}\n",
        "    print(f\"Final vocabulary size: {len(vocab)}\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/data.json'\n",
        "with open(file_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Use only training data for vocabulary\n",
        "training_data = data['training_data']\n",
        "\n",
        "# Build vocabulary using only training data with 1% minimum document frequency\n",
        "vocab_src = build_vocab(training_data, min_freq_ratio=0.01)\n",
        "\n",
        "def re_index_vocab(vocab):\n",
        "    new_vocab = {}\n",
        "    for i, token in enumerate(vocab.keys()):\n",
        "        new_vocab[token] = i\n",
        "    return new_vocab\n",
        "\n",
        "vocab_src = re_index_vocab(vocab_src)\n",
        "\n",
        "vocab_tgt = vocab_src.copy()\n",
        "\n",
        "# Save vocabularies\n",
        "with open(\"vocab_src.json\", \"w\") as f:\n",
        "    json.dump(vocab_src, f, indent=4)\n",
        "\n",
        "with open(\"vocab_tgt.json\", \"w\") as f:\n",
        "    json.dump(vocab_tgt, f, indent=4)\n",
        "\n",
        "print(\"Vocabularies created and saved successfully!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NMjWem1ci8X"
      },
      "source": [
        "## Hier Encoder, Decoder, Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjSnDEhqk1oX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGjt3FVL1E3F",
        "outputId": "6c7f0786-9255-409b-a177-3f9632febf05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0doml_vy1HwA",
        "outputId": "3518cd32-50fa-4248-e64a-b5cb48c298ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EtQJrKjJ1Jnz",
        "outputId": "9c693be2-eed3-4bc5-9c6f-c59093541001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Vocab size: 8603\n",
            "Epoch 1, Loss: 3.8689\n",
            "Epoch 2, Loss: 2.9240\n",
            "Epoch 3, Loss: 2.5205\n",
            "Epoch 4, Loss: 2.2017\n",
            "Epoch 5, Loss: 1.9351\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john jones\n",
            "True: john wilson\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: tim\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 5:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount township new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: \n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray smith\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.270\n",
            "ROUGE-2: 0.055\n",
            "ROUGE-L: 0.270\n",
            "New best model saved with ROUGE-L: 0.270\n",
            "Epoch 6, Loss: 1.6698\n",
            "Epoch 7, Loss: 1.4442\n",
            "Epoch 8, Loss: 1.2295\n",
            "Epoch 9, Loss: 1.0492\n",
            "Epoch 10, Loss: 0.9010\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john anderson\n",
            "True: john wilson\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount vernon new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: terry\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray lee\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.346\n",
            "ROUGE-2: 0.044\n",
            "ROUGE-L: 0.346\n",
            "New best model saved with ROUGE-L: 0.346\n",
            "Epoch 11, Loss: 0.7535\n",
            "Epoch 12, Loss: 0.6281\n",
            "Epoch 13, Loss: 0.5441\n",
            "Epoch 14, Loss: 0.4548\n",
            "Epoch 15, Loss: 0.3803\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: fc\n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: derrick\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray lee\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.321\n",
            "ROUGE-2: 0.064\n",
            "ROUGE-L: 0.321\n",
            "Epoch 16, Loss: 0.3334\n",
            "Epoch 17, Loss: 0.2842\n",
            "Epoch 18, Loss: 0.2491\n",
            "Epoch 19, Loss: 0.2212\n",
            "Epoch 20, Loss: 0.1974\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: fc\n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: derrick\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray lee\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.365\n",
            "ROUGE-2: 0.085\n",
            "ROUGE-L: 0.365\n",
            "New best model saved with ROUGE-L: 0.365\n",
            "Epoch 21, Loss: 0.1741\n",
            "Epoch 22, Loss: 0.1582\n",
            "Epoch 23, Loss: 0.1442\n",
            "Epoch 24, Loss: 0.1324\n",
            "Epoch 25, Loss: 0.1236\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: sofia\n",
            "True: sofia\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: name\n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: derrick\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray lee\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.374\n",
            "ROUGE-2: 0.084\n",
            "ROUGE-L: 0.374\n",
            "New best model saved with ROUGE-L: 0.374\n",
            "Epoch 26, Loss: 0.1147\n",
            "Epoch 27, Loss: 0.1088\n",
            "Epoch 28, Loss: 0.1027\n",
            "Epoch 29, Loss: 0.1022\n",
            "Epoch 30, Loss: 0.0938\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: name\n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: lionel\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray lewis\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.358\n",
            "ROUGE-2: 0.068\n",
            "ROUGE-L: 0.358\n",
            "Epoch 31, Loss: 0.0899\n",
            "Epoch 32, Loss: 0.0876\n",
            "Epoch 33, Loss: 0.0894\n",
            "Epoch 34, Loss: 0.0825\n",
            "Epoch 35, Loss: 0.0772\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: terry\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray perkins\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.369\n",
            "ROUGE-2: 0.091\n",
            "ROUGE-L: 0.369\n",
            "Epoch 36, Loss: 0.0784\n",
            "Epoch 37, Loss: 0.0765\n",
            "Epoch 38, Loss: 0.0719\n",
            "Epoch 39, Loss: 0.0764\n",
            "Epoch 40, Loss: 0.0725\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: name\n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: tom\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray bailey\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.337\n",
            "ROUGE-2: 0.090\n",
            "ROUGE-L: 0.337\n",
            "Epoch 41, Loss: 0.0704\n",
            "Epoch 42, Loss: 0.0704\n",
            "Epoch 43, Loss: 0.0657\n",
            "Epoch 44, Loss: 0.0679\n",
            "Epoch 45, Loss: 0.0643\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: name\n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount pleasant new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: tyrone\n",
            "True: tyrone\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray washington\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.382\n",
            "ROUGE-2: 0.080\n",
            "ROUGE-L: 0.382\n",
            "New best model saved with ROUGE-L: 0.382\n",
            "Epoch 46, Loss: 0.0645\n",
            "Epoch 47, Loss: 0.0645\n",
            "Epoch 48, Loss: 0.0638\n",
            "Epoch 49, Loss: 0.0644\n",
            "Epoch 50, Loss: 0.0575\n",
            "\n",
            "Validation Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: john wilson\n",
            "True: john wilson\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 1.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: \n",
            "True: sofia\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 3:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: jeff\n",
            "True: jeff american football\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Example 5:\n",
            "Predicted: name\n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: mount river new jersey\n",
            "True: mount laurel new jersey\n",
            "ROUGE-1: 0.750\n",
            "ROUGE-2: 0.333\n",
            "ROUGE-L: 0.750\n",
            "\n",
            "Example 7:\n",
            "Predicted: tom\n",
            "True: tyrone\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: ray washington\n",
            "True: raymond washington\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.373\n",
            "ROUGE-2: 0.079\n",
            "ROUGE-L: 0.373\n",
            "\n",
            "Test Evaluation:\n",
            "\n",
            "Example 1:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 2:\n",
            "Predicted: valencia high school\n",
            "True: catholic high school singapore\n",
            "ROUGE-1: 0.571\n",
            "ROUGE-2: 0.400\n",
            "ROUGE-L: 0.571\n",
            "\n",
            "Example 3:\n",
            "Predicted: paulo university\n",
            "True: minnesota golden\n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 4:\n",
            "Predicted: list people minnesota\n",
            "True: list people louisiana\n",
            "ROUGE-1: 0.667\n",
            "ROUGE-2: 0.500\n",
            "ROUGE-L: 0.667\n",
            "\n",
            "Example 5:\n",
            "Predicted: \n",
            "True: \n",
            "ROUGE-1: 0.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.000\n",
            "\n",
            "Example 6:\n",
            "Predicted: fc\n",
            "True: fc\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 7:\n",
            "Predicted: maryland\n",
            "True: maryland\n",
            "ROUGE-1: 1.000\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 1.000\n",
            "\n",
            "Example 8:\n",
            "Predicted: university northern\n",
            "True: university oxford\n",
            "ROUGE-1: 0.500\n",
            "ROUGE-2: 0.000\n",
            "ROUGE-L: 0.500\n",
            "\n",
            "Average ROUGE Scores:\n",
            "ROUGE-1: 0.417\n",
            "ROUGE-2: 0.081\n",
            "ROUGE-L: 0.417\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import json\n",
        "import random\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "SPECIAL_TOKENS = {\n",
        "    \"<pad>\": 0,  # Padding\n",
        "    \"<sos>\": 1,  # Start of sequence\n",
        "    \"<eos>\": 2,  # End of sequence\n",
        "    \"<unk>\": 3   # Unknown word\n",
        "}\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "EMB_DIM = 300 # 512\n",
        "HID_DIM = 300\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 50\n",
        "MAX_LEN = 10 # we can increase this\n",
        "MAX_LEN_SRC = 500\n",
        "TEACHER_FORCING_RATIO = 0.5\n",
        "\n",
        "# Text processing\n",
        "# def clean_text(text):\n",
        "#     \"\"\"Basic text cleaning\"\"\"\n",
        "#     if not isinstance(text, str):\n",
        "#         return \"\"\n",
        "#     text = text.lower().strip()\n",
        "#     text = re.sub(r'[^a-z0-9\\\\s]', '', text)  # Keep alphanumeric\n",
        "#     return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# Dataset\n",
        "class HeadlineDataset(Dataset):\n",
        "    def __init__(self, data, vocab_src):\n",
        "        self.data = data\n",
        "        self.vocab_src = vocab_src\n",
        "        self.vocab_tgt = vocab_src\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    #     # text = clean_text(self.data[idx]['text'])\n",
        "    #     text = (self.data[idx]['text'])\n",
        "    #     # title = clean_text(self.data[idx]['title'])\n",
        "    #     title = (self.data[idx]['title'])\n",
        "\n",
        "    #     src = [self.vocab_src['<sos>']] + \\\\n",
        "    #           [self.vocab_src.get(word, self.vocab_src['<unk>'])\\n",
        "    #            for word in text.split()[:MAX_LEN-1]] + \\\\n",
        "    #           [self.vocab_src['<eos>']]\n",
        "\n",
        "    #     tgt = [self.vocab_tgt['<sos>']] + \\\\n",
        "    #           [self.vocab_tgt.get(word, self.vocab_tgt['<unk>'])\\n",
        "    #            for word in title.split()[:MAX_LEN-1]] + \\\\n",
        "    #           [self.vocab_tgt['<eos>']]\n",
        "\n",
        "    #     return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "    #     text = clean_text(self.data[idx]['text'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      text = self.data[idx]['text']\n",
        "      title = self.data[idx]['title']\n",
        "\n",
        "      src = [self.vocab_src['<sos>']]\n",
        "      src += [self.vocab_src.get(word, self.vocab_src['<unk>']) for word in text.split()[:MAX_LEN_SRC-1]]\n",
        "      src.append(self.vocab_src['<eos>'])\n",
        "\n",
        "      vocab_size = len(self.vocab_src)\n",
        "      # Ensure all tokens are within valid range\n",
        "      src = [tok if tok < vocab_size else self.vocab_src['<unk>'] for tok in src]\n",
        "\n",
        "      # Repeat for tgt\n",
        "      tgt = [self.vocab_tgt.get(word, self.vocab_tgt['<unk>']) for word in title.split()[:MAX_LEN-1]]\n",
        "      tgt = [tok if tok < vocab_size else self.vocab_tgt['<unk>'] for tok in tgt]\n",
        "\n",
        "      return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_padded = pad_sequence(src_batch, padding_value=SPECIAL_TOKENS['<pad>'], batch_first=True)\n",
        "    tgt_padded = pad_sequence(tgt_batch, padding_value=SPECIAL_TOKENS['<pad>'], batch_first=True)\n",
        "    return src_padded, tgt_padded\n",
        "\n",
        "def check_dataset_indices(dataset, vocab_size):\n",
        "    for i in range(len(dataset)):\n",
        "        src, tgt = dataset[i]\n",
        "        if src.max() >= vocab_size or tgt.max() >= vocab_size:\n",
        "            print(f\"Invalid indices in sample {i}\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Model Architecture\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout_rate= 0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True,bidirectional=True)\n",
        "\n",
        "        # Linear layer to combine bidirectional outputs for the decoder\n",
        "        self.fc = nn.Linear(hid_dim * 2, hid_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden_forward = hidden[0, :, :]\n",
        "        hidden_backward = hidden[1, :, :]\n",
        "        hidden_combined = torch.cat((hidden_forward, hidden_backward), dim=1)\n",
        "        hidden_transformed = torch.tanh(self.fc(hidden_combined))\n",
        "        hidden_for_decoder = hidden_transformed.unsqueeze(0)\n",
        "\n",
        "        return outputs, hidden_for_decoder\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, ff_dim=512, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        # Existing components\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
        "\n",
        "        # # New FFNN components\n",
        "        # self.ffnn = nn.Sequential(\n",
        "        #     nn.Linear(hid_dim, ff_dim),\\n",
        "        #     nn.ReLU(),\\n",
        "        #     nn.Dropout(dropout_rate),\\n",
        "        #     nn.Linear(ff_dim, hid_dim),\\n",
        "        #     nn.ReLU()\\n",
        "        # )\n",
        "\n",
        "        # Final output layer\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden):\n",
        "        # Existing RNN processing\n",
        "        embedded = self.dropout(self.embedding(input_token))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "\n",
        "        # New FFNN processing\n",
        "        # ff_output = self.ffnn(output)  # Shape: [batch_size, seq_len, hid_dim]\\n",
        "\n",
        "        # Final projection to vocabulary\n",
        "        # prediction = self.fc_out(ff_output.squeeze(1))\\n",
        "        prediction = self.fc_out(output.squeeze(1))\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, vocab_src, max_len=50):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "        self.vocabulary = vocab_src\n",
        "\n",
        "    def forward(self, src, tgt=None, teacher_forcing_ratio=1):\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        # Define target length - use tgt length during training, max_len during inference\n",
        "        tgt_len = tgt.shape[1] if tgt is not None else self.max_len\n",
        "\n",
        "        # Define vocabulary size from decoder output layer\n",
        "        vocab_size = self.decoder.fc_out.out_features  # Fixed: was fc.out_features\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # First decoder input is the <SOS> token\n",
        "        input_token = torch.tensor([[self.vocabulary[\"<sos>\"]] * batch_size], device=self.device).T\n",
        "        # For batching: input_token shape should be [batch_size, 1]\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            # Pass through decoder\n",
        "            # Note: Removed the encoder_outputs argument as it's not in decoder's forward signature\n",
        "            prediction, hidden = self.decoder(input_token, hidden)\n",
        "\n",
        "            # Store prediction\n",
        "            outputs[:, t, :] = prediction\n",
        "\n",
        "            # Teacher forcing: decide whether to use real target tokens\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_forcing and tgt is not None:\n",
        "                # Use actual next token as next input\n",
        "                input_token = tgt[:, t].unsqueeze(1)\n",
        "            else:\n",
        "                # Use best predicted token\n",
        "                top1 = prediction.argmax(1).unsqueeze(1)\n",
        "                input_token = top1\n",
        "\n",
        "            # Stop if all sequences in batch have generated EOS\n",
        "            if tgt is None and (input_token == self.vocabulary[\"<eos>\"]).all():\n",
        "                break\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def evaluate_rouge(model, dataset, vocab_src, vocab_tgt, num_examples=8):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    idx2word_tgt = {v:k for k,v in vocab_tgt.items()}\n",
        "\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    examples = []\n",
        "\n",
        "    # Get special token IDs\n",
        "    special_tokens = {vocab_tgt[\"<pad>\"], vocab_tgt[\"<sos>\"], vocab_tgt[\"<eos>\"], vocab_tgt[\"<unk>\"]}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(min(100, len(dataset))):  # Evaluate on max 100 examples\n",
        "            src, tgt = dataset[i]\n",
        "            src = src.unsqueeze(0).to(model.device)\n",
        "\n",
        "            # Generate output with the model (no target provided for inference)\n",
        "            outputs = model(src, tgt=None, teacher_forcing_ratio=0)\n",
        "\n",
        "            # Get the predictions (batch_size=1, so we take index 0)\n",
        "            output_tokens = outputs[0].argmax(dim=1).cpu().numpy()\n",
        "\n",
        "            # Convert tokens to words, filtering out special tokens\n",
        "            pred = ' '.join([idx2word_tgt[idx] for idx in output_tokens\\n",
        "                           if idx not in special_tokens])\n",
        "\n",
        "            true = ' '.join([idx2word_tgt[idx.item()] for idx in tgt\\n",
        "                           if idx.item() not in special_tokens])\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            rouge_scores = scorer.score(true, pred)\n",
        "            for key in scores:\n",
        "                scores[key].append(rouge_scores[key].fmeasure)\n",
        "\n",
        "            # Save examples\n",
        "            if len(examples) < num_examples:\n",
        "                examples.append((pred, true, rouge_scores))\n",
        "\n",
        "    # Print examples\n",
        "    for i, (pred, true, rouge) in enumerate(examples):\n",
        "        print(f\"\\\\nExample {i+1}:\")\n",
        "        print(f\"Predicted: {pred}\")\n",
        "        print(f\"True: {true}\")\n",
        "        print(f\"ROUGE-1: {rouge['rouge1'].fmeasure:.3f}\")\n",
        "        print(f\"ROUGE-2: {rouge['rouge2'].fmeasure:.3f}\")\n",
        "        print(f\"ROUGE-L: {rouge['rougeL'].fmeasure:.3f}\")\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_scores = {k: np.mean(v) for k,v in scores.items()}\n",
        "    print(\"\\\\nAverage ROUGE Scores:\")\n",
        "    print(f\"ROUGE-1: {avg_scores['rouge1']:.3f}\")\n",
        "    print(f\"ROUGE-2: {avg_scores['rouge2']:.3f}\")\n",
        "    print(f\"ROUGE-L: {avg_scores['rougeL']:.3f}\")\n",
        "\n",
        "    return avg_scores\n",
        "\n",
        "\n",
        "def train_model(data, vocab_src):\n",
        "    # Build vocabularies\n",
        "    # vocab_src = build_vocab(data)\n",
        "    print(f\"Vocab size: {len(vocab_src)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_data = HeadlineDataset(data['training_data'], vocab_src)\n",
        "    val_data = HeadlineDataset(data['validation_data'], vocab_src)\n",
        "    test_data = HeadlineDataset(data['test_data'], vocab_src)\n",
        "\n",
        "    assert check_dataset_indices(train_data, len(vocab_src)), \"Invalid indices found in training data\"\n",
        "\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\\n",
        "                            shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "\n",
        "\n",
        "    # Initialize model with updated parameters\n",
        "    encoder = Encoder(len(vocab_src), EMB_DIM, HID_DIM)\n",
        "    decoder = Decoder(len(vocab_src), EMB_DIM, HID_DIM)\n",
        "    model = Seq2Seq(encoder, decoder, device, vocab_src, max_len=MAX_LEN).to(device)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=SPECIAL_TOKENS['<pad>'])\n",
        "\n",
        "\n",
        "    vocab_size = len(vocab_src)\n",
        "    for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "        if (src >= vocab_size).any():\n",
        "            print(f\"Found out-of-bounds indices in batch {batch_idx}\")\n",
        "            print(f\"Max index: {src.max().item()}, Vocab size: {vocab_size}\")\n",
        "            # Fix the indices by capping them\n",
        "            src[src >= vocab_size] = vocab_src[\"<unk>\"]\n",
        "\n",
        "        # Same check for target\n",
        "        if (tgt >= vocab_size).any():\n",
        "            tgt[tgt >= vocab_size] = vocab_src[\"<unk>\"]\n",
        "\n",
        "    # Training loop\n",
        "    best_val_score = 0\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Pass tgt for teacher forcing\n",
        "            output = model(src, tgt, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
        "\n",
        "            # Reshape output and target for loss calculation\n",
        "            # output: [batch_size, tgt_len, vocab_size]\n",
        "            # Target should exclude <sos> token (first token)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss/len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Evaluate every 5 epochs\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(\"\\\\nValidation Evaluation:\")\n",
        "            val_scores = evaluate_rouge(model, val_data, vocab_src, vocab_src)\n",
        "\n",
        "            # Save the best model\n",
        "            if val_scores['rougeL'] > best_val_score:\n",
        "                best_val_score = val_scores['rougeL']\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': avg_loss,\n",
        "                    'vocab': vocab_src\n",
        "                }, 'best_headline_generator.pth')\n",
        "                print(f\"New best model saved with ROUGE-L: {best_val_score:.3f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\\\nTest Evaluation:\")\n",
        "    test_scores = evaluate_rouge(model, test_data, vocab_src, vocab_src)\n",
        "\n",
        "    # Save final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab_src,\n",
        "        'config': {\n",
        "            'emb_dim': EMB_DIM,\n",
        "            'hid_dim': HID_DIM,\n",
        "            'max_len': MAX_LEN,\n",
        "            'max_len_src' : MAX_LEN_SRC\n",
        "        }\n",
        "    }, 'final_headline_generator.pth')\n",
        "\n",
        "    return model, val_scores, test_scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data - Make sure this matches your actual data loading code\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    model, val_scores, test_scores = train_model(data, vocab_src)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16AMDEIziaUU"
      },
      "outputs": [],
      "source": [
        "print(\"Max index in vocab:\", max(vocab_src.values()))\n",
        "print(\"Vocab size:\", len(vocab_src))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxNp7WWC2z-d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y4IkG9nd_5U"
      },
      "outputs": [],
      "source": [
        "train_data = HeadlineDataset(data['training_data'], vocab_src)\n",
        "for i in range(5):\n",
        "    src, tgt = train_data[i]\n",
        "    print(tgt)\n",
        "    assert src.max() < len(vocab_src), f\"Invalid src index in sample {i}\"\n",
        "    assert tgt.max() < len(vocab_src), f\"Invalid tgt index in sample {i}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsRpgbOnSOK_"
      },
      "source": [
        "## Transfomer Fine tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7BVCyi6SLu0",
        "outputId": "9afb30bb-66e7-4a75-a665-8e069c9ff24b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSV files...\n",
            "Creating datasets...\n",
            "Loading training data...\n",
            "Loading test data...\n",
            "Saving results to processed_data.json...\n",
            "Processing complete! Data saved to processed_data.json\n",
            "Processed 13379 training items, 100 test items, and 500 validation items\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_datasets(train_df, test_df, random_seed=42):\n",
        "    \"\"\"Create training, validation, and test datasets without text processing\"\"\"\n",
        "    print(\"Creating datasets...\")\n",
        "\n",
        "    # Create a random permutation for validation split\n",
        "    np.random.seed(random_seed)\n",
        "    shuffled = np.random.permutation(len(train_df))\n",
        "    validation_indices = set(shuffled[:500])  # Take 500 samples for validation\n",
        "\n",
        "    training_data = []\n",
        "    validation_data = []\n",
        "    test_data = []\n",
        "\n",
        "    # Extract training and validation data\n",
        "    print(\"Loading training data...\")\n",
        "    for index, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Extracting training data\"):\n",
        "        item = {'text': row['text'], 'title': row['title']}\n",
        "        if index in validation_indices:\n",
        "            validation_data.append(item)\n",
        "        else:\n",
        "            training_data.append(item)\n",
        "\n",
        "    # Extract test data\n",
        "    print(\"Loading test data...\")\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Extracting test data\"):\n",
        "        test_data.append({'text': row['text'], 'title': row['title']})\n",
        "\n",
        "    return training_data, test_data, validation_data\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Load data\n",
        "    print(\"Loading CSV files...\")\n",
        "    try:\n",
        "        train_df = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "        test_df = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Please ensure train.csv and test.csv files are in the current directory.\")\n",
        "        return\n",
        "\n",
        "    # Create datasets\n",
        "    training_data, test_data, validation_data = create_datasets(train_df, test_df)\n",
        "\n",
        "    # Save results\n",
        "    data_dict = {\n",
        "        'training_data': training_data,\n",
        "        'test_data': test_data,\n",
        "        'validation_data': validation_data,\n",
        "    }\n",
        "\n",
        "    output_path = 'processed_data.json'\n",
        "    print(f\"Saving results to {output_path}...\")\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(data_dict, f, indent=4)\n",
        "\n",
        "    print(f\"Processing complete! Data saved to {output_path}\")\n",
        "    print(f\"Processed {len(training_data)} training items, {len(test_data)} test items, and {len(validation_data)} validation items\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZTM-SqgUFwI",
        "outputId": "f47f5ce8-2335-4c05-be95-c0194e07eccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Mx-VRMNScbN",
        "outputId": "c27dfbd0-ec0f-4bea-e4a9-d462740fc37a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-8-200dad2ec6c5>:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Greedy Results:\n",
            "rouge1: 0.9511\n",
            "rouge2: 0.8493\n",
            "rougeL: 0.9494\n",
            "\n",
            "Beam Search Results:\n",
            "rouge1: 0.9577\n",
            "rouge2: 0.8622\n",
            "rougeL: 0.9561\n",
            "\n",
            "Results Comparison:\n",
            "Metric     Greedy     Beam Search\n",
            "------------------------------\n",
            "rouge1     0.9511      0.9577\n",
            "rouge2     0.8493      0.8622\n",
            "rougeL     0.9494      0.9561\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Load pretrained model\n",
        "model_name = \"google-t5/t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# 2. Prepare datasets\n",
        "def convert_to_hf_dataset(data, tokenizer, max_input_length=512, max_target_length=64):\n",
        "    texts = [item['text'] for item in data]\n",
        "    titles = [item['title'] for item in data]\n",
        "\n",
        "    inputs = [\"summarize: \" + text for text in texts]\n",
        "\n",
        "    input_encodings = tokenizer(inputs, max_length=max_input_length,\\n",
        "                                truncation=True, padding=\"max_length\")\n",
        "\n",
        "    target_encodings = tokenizer(titles, max_length=max_target_length,\\n",
        "                                truncation=True, padding=\"max_length\")\n",
        "\n",
        "    dataset_dict = {\\n",
        "        \"input_ids\": input_encodings.input_ids,\\n",
        "        \"attention_mask\": input_encodings.attention_mask,\\n",
        "        \"labels\": target_encodings.input_ids\\n",
        "    }\n",
        "\n",
        "    for i in range(len(dataset_dict[\"labels\"])):\n",
        "        dataset_dict[\"labels\"][i] = [\\n",
        "            (l if l != tokenizer.pad_token_id else -100) for l in dataset_dict[\"labels\"][i]\\n",
        "        ]\n",
        "\n",
        "    return Dataset.from_dict(dataset_dict)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = convert_to_hf_dataset(data['training_data'], tokenizer)\n",
        "val_dataset = convert_to_hf_dataset(data['validation_data'], tokenizer)\n",
        "test_dataset = convert_to_hf_dataset(data['test_data'], tokenizer)\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "# 3. Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        results = scorer.score(label, pred)\n",
        "        scores['rouge1'] += results['rouge1'].fmeasure\n",
        "        scores['rouge2'] += results['rouge2'].fmeasure\n",
        "        scores['rougeL'] += results['rougeL'].fmeasure\n",
        "\n",
        "    scores = {k: v / len(decoded_preds) for k, v in scores.items()}\n",
        "    return scores\n",
        "\n",
        "# 4. Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5-headline-generator\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True if torch.cuda.is_available() else False,\n",
        "    logging_steps=100,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# 5. Create trainer and train\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# 6. Generate headlines and evaluate\n",
        "def generate_and_evaluate(model, tokenizer, test_data, use_beam_search=False, beam_width=4):\n",
        "    model.eval()\n",
        "    batch_size = 16\n",
        "    num_samples = len(test_data)\n",
        "    predictions = []\n",
        "    references = [item['title'] for item in test_data]\n",
        "\n",
        "    for i in tqdm(range(0, num_samples, batch_size)):\n",
        "        batch_data = test_data[i:min(i+batch_size, num_samples)]\n",
        "        input_texts = [\"summarize: \" + item['text'] for item in batch_data]\n",
        "        inputs = tokenizer(input_texts, max_length=512, truncation=True,\\n",
        "                          padding=True, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        if use_beam_search:\n",
        "            outputs = model.generate(\\n",
        "                **inputs,\\n",
        "                max_length=64,\\n",
        "                num_beams=beam_width,\\n",
        "                early_stopping=True\\n",
        "            )\n",
        "        else:\n",
        "            outputs = model.generate(\\n",
        "                **inputs,\\n",
        "                max_length=64,\\n",
        "                num_beams=1\\n",
        "            )\n",
        "\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        predictions.extend(decoded_outputs)\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        results = scorer.score(ref, pred)\n",
        "        scores['rouge1'] += results['rouge1'].fmeasure\n",
        "        scores['rouge2'] += results['rouge2'].fmeasure\n",
        "        scores['rougeL'] += results['rougeL'].fmeasure\n",
        "\n",
        "    avg_scores = {k: v / len(predictions) for k, v in scores.items()}\n",
        "\n",
        "    print(f\"\\\\n{'Beam Search' if use_beam_search else 'Greedy'} Results:\")\n",
        "    for k, v in avg_scores.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    return predictions, avg_scores\n",
        "\n",
        "# Generate with greedy search\n",
        "greedy_preds, greedy_scores = generate_and_evaluate(model, tokenizer, data['test_data'], use_beam_search=False)\n",
        "\n",
        "# Generate with beam search\n",
        "beam_preds, beam_scores = generate_and_evaluate(model, tokenizer, data['test_data'], use_beam_search=True, beam_width=4)\n",
        "\n",
        "# Compare results\n",
        "print(\"\\\\nResults Comparison:\")\n",
        "print(f\"{'Metric':<10} {'Greedy':<10} {'Beam Search':<10}\")\n",
        "print(\"-\" * 30)\n",
        "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "    print(f\"{metric:<10} {greedy_scores[metric]:.4f}      {beam_scores[metric]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEf_BrbafqJb"
      },
      "source": [
        "## C2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU2kZKz1upIk",
        "outputId": "3936a70f-c07b-49f8-ff4c-4b0fb3fb45c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e9c74e8bb83afbb59b4481a77ce4d06383cd6e3537c3689c8903db35be52aace\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.1)\n",
            "Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet])\n",
            "  Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
            "Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-xet\n",
            "Successfully installed hf-xet-1.0.3\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install hf_xet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BHP7Sx3Qftxo",
        "outputId": "597c5c14-dd7d-467c-f0cb-7a0d12f053bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.4\n",
            "Available GPU memory: 14.74 GB\n",
            "Converted /content/drive/MyDrive/test.csv to /content/test.json\n",
            "Loaded 100 articles with reference titles\n",
            "\n",
            "Processing model: google/flan-t5-base\n",
            "  GPU memory allocated: 947.43 MB\n",
            "  GPU memory reserved: 1014.00 MB\n",
            "  Using prompt: \"Give a title for the following article: \"\n",
            "  Processed 10/100 articles...\n",
            "  Processed 20/100 articles...\n",
            "  Processed 30/100 articles...\n",
            "  Processed 40/100 articles...\n",
            "  Processed 50/100 articles...\n",
            "  Processed 60/100 articles...\n",
            "  Processed 70/100 articles...\n",
            "  Processed 80/100 articles...\n",
            "  Processed 90/100 articles...\n",
            "  Generation completed in 25.35 seconds for 100 articles\n",
            "  Average time per article: 0.25 seconds\n",
            "    ROUGE-1: 0.8556\n",
            "    ROUGE-2: 0.6665\n",
            "    ROUGE-L: 0.8556\n",
            "    Average: 0.7926\n",
            "  Using prompt: \"Based on this given text create an appropriate title : \"\n",
            "  Processed 10/100 articles...\n",
            "  Processed 20/100 articles...\n",
            "  Processed 30/100 articles...\n",
            "  Processed 40/100 articles...\n",
            "  Processed 50/100 articles...\n",
            "  Processed 60/100 articles...\n",
            "  Processed 70/100 articles...\n",
            "  Processed 80/100 articles...\n",
            "  Processed 90/100 articles...\n",
            "  Generation completed in 27.68 seconds for 100 articles\n",
            "  Average time per article: 0.28 seconds\n",
            "    ROUGE-1: 0.8320\n",
            "    ROUGE-2: 0.6416\n",
            "    ROUGE-L: 0.8280\n",
            "    Average: 0.7672\n",
            "  GPU memory freed up\n",
            "\n",
            "Processing model: google/flan-t5-large\n",
            "  GPU memory allocated: 3140.61 MB\n",
            "  GPU memory reserved: 3154.00 MB\n",
            "  Using prompt: \"Give a title for the following article: \"\n",
            "  Processed 10/100 articles...\n",
            "  Processed 20/100 articles...\n",
            "  Processed 30/100 articles...\n",
            "  Processed 40/100 articles...\n",
            "  Processed 50/100 articles...\n",
            "  Processed 60/100 articles...\n",
            "  Processed 70/100 articles...\n",
            "  Processed 80/100 articles...\n",
            "  Processed 90/100 articles...\n",
            "  Generation completed in 55.27 seconds for 100 articles\n",
            "  Average time per article: 0.55 seconds\n",
            "    ROUGE-1: 0.8786\n",
            "    ROUGE-2: 0.6423\n",
            "    ROUGE-L: 0.8786\n",
            "    Average: 0.7999\n",
            "  Using prompt: \"Based on this given text create an appropriate title : \"\n",
            "  Processed 10/100 articles...\n",
            "  Processed 20/100 articles...\n",
            "  Processed 30/100 articles...\n",
            "  Processed 40/100 articles...\n",
            "  Processed 50/100 articles...\n",
            "  Processed 60/100 articles...\n",
            "  Processed 70/100 articles...\n",
            "  Processed 80/100 articles...\n",
            "  Processed 90/100 articles...\n",
            "  Generation completed in 58.74 seconds for 100 articles\n",
            "  Average time per article: 0.59 seconds\n",
            "    ROUGE-1: 0.9024\n",
            "    ROUGE-2: 0.6938\n",
            "    ROUGE-L: 0.9024\n",
            "    Average: 0.8328\n",
            "  GPU memory freed up\n",
            "\n",
            "===== FINAL ROUGE SCORES BY PROMPT =====\n",
            "\n",
            "Prompt: \"Give a title for the following article: \"\n",
            "  google/flan-t5-base:\n",
            "    ROUGE-1: 0.8556\n",
            "    ROUGE-2: 0.6665\n",
            "    ROUGE-L: 0.8556\n",
            "    Average: 0.7926\n",
            "  google/flan-t5-large:\n",
            "    ROUGE-1: 0.8786\n",
            "    ROUGE-2: 0.6423\n",
            "    ROUGE-L: 0.8786\n",
            "    Average: 0.7999\n",
            "  AVERAGE ACROSS MODELS:\n",
            "    ROUGE-1: 0.8671\n",
            "    ROUGE-2: 0.6544\n",
            "    ROUGE-L: 0.8671\n",
            "    Average: 0.7962\n",
            "\n",
            "Prompt: \"Based on this given text create an appropriate title : \"\n",
            "  google/flan-t5-base:\n",
            "    ROUGE-1: 0.8320\n",
            "    ROUGE-2: 0.6416\n",
            "    ROUGE-L: 0.8280\n",
            "    Average: 0.7672\n",
            "  google/flan-t5-large:\n",
            "    ROUGE-1: 0.9024\n",
            "    ROUGE-2: 0.6938\n",
            "    ROUGE-L: 0.9024\n",
            "    Average: 0.8328\n",
            "  AVERAGE ACROSS MODELS:\n",
            "    ROUGE-1: 0.8672\n",
            "    ROUGE-2: 0.6677\n",
            "    ROUGE-L: 0.8652\n",
            "    Average: 0.8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# First, check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
        "\n",
        "# Step 1: Convert CSV to JSON\n",
        "def convert_csv_to_json(csv_path, json_path):\n",
        "    data = pd.read_csv(csv_path)\n",
        "    data.to_json(json_path, orient='records', lines=True)\n",
        "    print(f\"Converted {csv_path} to {json_path}\")\n",
        "    return json_path\n",
        "\n",
        "# Step 2: Load articles from JSON\n",
        "def load_articles_from_json(json_path):\n",
        "    with open(json_path, 'r') as file:\n",
        "        json_data = [json.loads(line) for line in file]\n",
        "\n",
        "    # Identify article and title fields\n",
        "    possible_article_fields = ['article', 'body', 'content', 'text']\n",
        "    possible_title_fields = ['title', 'headline', 'header']\n",
        "\n",
        "    article_field = next((f for f in possible_article_fields if f in json_data[0]), None)\n",
        "    title_field = next((f for f in possible_title_fields if f in json_data[0]), None)\n",
        "\n",
        "    if not article_field or not title_field:\n",
        "        raise ValueError(f\"Could not find article or title fields. Available: {list(json_data[0].keys())}\")\n",
        "\n",
        "    articles = [item[article_field] for item in json_data if article_field in item]\n",
        "    reference_titles = [item[title_field] for item in json_data if title_field in item]\n",
        "\n",
        "    return articles, reference_titles\n",
        "\n",
        "# Step 3: Generate titles using LLMs\n",
        "def generate_titles(articles, model, tokenizer, prompt_prefix, device):\n",
        "    generated_titles = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, article in enumerate(articles):\n",
        "        if i % 10 == 0 and i > 0:\n",
        "            print(f\"  Processed {i}/{len(articles)} articles...\")\n",
        "\n",
        "        prompt = prompt_prefix + article\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        # Move inputs to the same device as the model\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate with CUDA acceleration if available\n",
        "        outputs = model.generate(**inputs, max_length=50, num_beams=5, early_stopping=True)\n",
        "        generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_titles.append(generated_title)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"  Generation completed in {elapsed_time:.2f} seconds for {len(articles)} articles\")\n",
        "    print(f\"  Average time per article: {elapsed_time/len(articles):.2f} seconds\")\n",
        "\n",
        "    return generated_titles\n",
        "\n",
        "# Step 4: Calculate ROUGE scores\n",
        "def calculate_rouge_scores(reference_titles, generated_titles):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    for ref_title, gen_title in zip(reference_titles, generated_titles):\n",
        "        scores = scorer.score(ref_title, gen_title)\n",
        "\n",
        "        # Extract F1 scores for each ROUGE metric\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    # Calculate average ROUGE scores\n",
        "    avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores) if rouge1_scores else 0\n",
        "    avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores) if rouge2_scores else 0\n",
        "    avg_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0\n",
        "\n",
        "    return {\\n",
        "        'rouge1': avg_rouge1,\\n",
        "        'rouge2': avg_rouge2,\\n",
        "        'rougeL': avg_rougeL,\\n",
        "        'average': (avg_rouge1 + avg_rouge2 + avg_rougeL) / 3\\n",
        "    }\n",
        "\n",
        "# Main function to execute all steps\n",
        "def main():\n",
        "    # Configure paths according to Kaggle environment\n",
        "    csv_path = '/content/drive/MyDrive/test.csv'\n",
        "    json_path = '/content/test.json'\n",
        "\n",
        "    # Models to use - both base and large as per task requirements\n",
        "    model_names = ['google/flan-t5-base', 'google/flan-t5-large']\n",
        "\n",
        "    # Define prompt variations for title generation\n",
        "    prompt_variations = [\\n",
        "        \"Give a title for the following article: \",\\n",
        "        \"Based on this given text create an appropriate title : \"\\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Step 1: Convert CSV to JSON\n",
        "        json_path = convert_csv_to_json(csv_path, json_path)\n",
        "\n",
        "        # Step 2: Load articles and reference titles from JSON\n",
        "        articles, reference_titles = load_articles_from_json(json_path)\n",
        "        print(f\"Loaded {len(articles)} articles with reference titles\")\n",
        "\n",
        "        # Results dictionary to store scores by model and prompt\n",
        "        results = {}\n",
        "\n",
        "        for model_name in model_names:\n",
        "            print(f\"\\\\nProcessing model: {model_name}\")\n",
        "            # Load model with CUDA support\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "            model = model.to(device)  # Move model to GPU if available\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            # Print memory usage after loading the model\n",
        "            if device.type == 'cuda':\n",
        "                print(f\"  GPU memory allocated: {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "                print(f\"  GPU memory reserved: {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")\n",
        "\n",
        "            model_results = {}\n",
        "\n",
        "            for prompt in prompt_variations:\n",
        "                print(f\"  Using prompt: \\\"{prompt}\\\"\")\n",
        "\n",
        "                # Step 3: Generate titles with this prompt\n",
        "                generated_titles = generate_titles(articles, model, tokenizer, prompt, device)\n",
        "\n",
        "                # Step 4: Calculate ROUGE scores for this prompt\n",
        "                scores = calculate_rouge_scores(reference_titles, generated_titles)\n",
        "                model_results[prompt] = scores\n",
        "\n",
        "                print(f\"    ROUGE-1: {scores['rouge1']:.4f}\")\n",
        "                print(f\"    ROUGE-2: {scores['rouge2']:.4f}\")\n",
        "                print(f\"    ROUGE-L: {scores['rougeL']:.4f}\")\n",
        "                print(f\"    Average: {scores['average']:.4f}\")\n",
        "\n",
        "            results[model_name] = model_results\n",
        "\n",
        "            # Clear GPU memory after using each model\n",
        "            if device.type == 'cuda':\n",
        "                del model\n",
        "                torch.cuda.empty_cache()\n",
        "                print(f\"  GPU memory freed up\")\n",
        "\n",
        "        # Print final summary of ROUGE scores for each prompt\n",
        "        print(\"\\\\n===== FINAL ROUGE SCORES BY PROMPT =====\")\n",
        "\n",
        "        for prompt in prompt_variations:\n",
        "            print(f\"\\\\nPrompt: \\\"{prompt}\\\"\")\n",
        "\n",
        "            # Calculate average scores across models for this prompt\n",
        "            rouge1_total = sum(results[model][prompt]['rouge1'] for model in model_names)\n",
        "            rouge2_total = sum(results[model][prompt]['rouge2'] for model in model_names)\n",
        "            rougeL_total = sum(results[model][prompt]['rougeL'] for model in model_names)\n",
        "            avg_total = sum(results[model][prompt]['average'] for model in model_names)\n",
        "\n",
        "            for model in model_names:\n",
        "                scores = results[model][prompt]\n",
        "                print(f\"  {model}:\")\n",
        "                print(f\"    ROUGE-1: {scores['rouge1']:.4f}\")\n",
        "                print(f\"    ROUGE-2: {scores['rouge2']:.4f}\")\n",
        "                print(f\"    ROUGE-L: {scores['rougeL']:.4f}\")\n",
        "                print(f\"    Average: {scores['average']:.4f}\")\n",
        "\n",
        "            # Print average across models for this prompt\n",
        "            model_count = len(model_names)\n",
        "            print(f\"  AVERAGE ACROSS MODELS:\")\n",
        "            print(f\"    ROUGE-1: {rouge1_total/model_count:.4f}\")\n",
        "            print(f\"    ROUGE-2: {rouge2_total/model_count:.4f}\")\n",
        "            print(f\"    ROUGE-L: {rougeL_total/model_count:.4f}\")\n",
        "            print(f\"    Average: {avg_total/model_count:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT86gqC3031_",
        "outputId": "1006e19d-91e6-4e66-9e05-294cc2960e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emDc8SfBOM1S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s02yrSoUcfxj",
        "6NMjWem1ci8X",
        "NoOz_saM55Gn"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
